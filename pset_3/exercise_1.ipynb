{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "Execute the following in the terminal before running any notebooks:\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: 1000 Alzheimer's disease and 1000 cancer papers from PubMed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('21473028', 'lupus'),\n",
       " ('21210563', 'lupus'),\n",
       " ('18958642', 'lupus'),\n",
       " ('18202459', 'lupus'),\n",
       " ('17822285', 'lupus'),\n",
       " ('17642789', 'lupus'),\n",
       " ('17642773', 'lupus'),\n",
       " ('17642626', 'lupus'),\n",
       " ('17642623', 'lupus'),\n",
       " ('17491665', 'lupus')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query Entrez API for PubMed IDs given search term and year\n",
    "import requests\n",
    "\n",
    "def get_pmids(query, year, retmax):\n",
    "    # Define efetch URL\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={query}+AND+{year}[pdat]&retmode=json&retmax={retmax}\"\n",
    "    \n",
    "    # Query the Entrez API\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()  # Raise an exception for HTTP error responses\n",
    "\n",
    "    # Grab the list of PMIDs\n",
    "    pmids = r.json()[\"esearchresult\"][\"idlist\"]\n",
    "    \n",
    "    # Return list of PMIDs with associated queries\n",
    "    return [(pmid, query) for pmid in pmids]\n",
    "\n",
    "# Test (featuring House MD; it is, unfortunately, never lupus)\n",
    "get_pmids(\"lupus\", \"2004\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Entrez for metadata of a PubMed paper given its PMID\n",
    "from lxml import etree\n",
    "\n",
    "def get_full_abstract(abstract_elements):\n",
    "    # Concatenate all abstract text elements to form the full abstract\n",
    "    full_abstract = ' '.join([abstract_elem.xpath(\"string()\") for abstract_elem in abstract_elements])\n",
    "    return full_abstract.strip()\n",
    "\n",
    "def get_metadata(pmids_with_queries):\n",
    "    # Convert list of PMIDs to a string for POST request\n",
    "    pmids_string = \",\".join([pmid for pmid, _ in pmids_with_queries])\n",
    "\n",
    "    # Define parameters for payload\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': pmids_string,\n",
    "        'retmode': 'xml'\n",
    "    }\n",
    "\n",
    "    # Query the Entrez API\n",
    "    r = requests.post(url, params)\n",
    "    r.raise_for_status()  # Raise an exception for HTTP error responses\n",
    "\n",
    "    # Parse the entire XML response\n",
    "    doc = etree.fromstring(r.content) # Use r.content for binary response\n",
    "    papers_dict = {}\n",
    "\n",
    "    # Iterate through each article in the entire response\n",
    "    for pmid, query in pmids_with_queries:\n",
    "        # Find the article node that corresponds to the current PMID\n",
    "        article = doc.xpath(f\".//PubmedArticle[MedlineCitation/PMID/text()='{pmid}']\")[0]\n",
    "\n",
    "        # Extract \"ArticleTitle\" for this article\n",
    "        title = article.findtext(\".//ArticleTitle\")\n",
    "        \n",
    "        # Grab all tags named \"AbstractText\" for this article\n",
    "        abstract_elements = article.xpath(\".//AbstractText\")\n",
    "        \n",
    "        # Build full abstract from tags\n",
    "        full_abstract = get_full_abstract(abstract_elements)\n",
    "        \n",
    "        # Populate paper dictionary\n",
    "        papers_dict[pmid] = {\n",
    "            \"ArticleTitle\": title,\n",
    "            \"AbstractText\": full_abstract,\n",
    "            \"query\": query\n",
    "        } \n",
    "\n",
    "    return papers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all 2000 papers and save metadata to JSON file\n",
    "import json\n",
    " \n",
    "pmids_with_queries = get_pmids(\"alzheimers\", \"2023\", 1000) + get_pmids(\"cancer\", \"2023\", 1000)\n",
    "\n",
    "papers_dict = get_metadata(pmids_with_queries)\n",
    "\n",
    "with open(\"papers_dict.json\", \"w\") as f:\n",
    "    json.dump(papers_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following PMIDs appear in both the Alzheimer's and the cancer sets of papers: ['37943296', '37937963', '37936448']\n"
     ]
    }
   ],
   "source": [
    "# Find overlap in PMIDs between two queries\n",
    "pmids = [pmid[0] for pmid in pmids_with_queries] # Extract only PMIDs\n",
    "split = len(pmids) // 2\n",
    "alzheimers, cancer = pmids[split:], pmids[:split] # Split PMIDs into queries\n",
    "\n",
    "def intersection(list_1, list_2):\n",
    "    overlap = [value for value in list_1 if value in list_2]\n",
    "    return overlap\n",
    "\n",
    "print(f\"The following PMIDs appear in both the Alzheimer's and the cancer sets of papers: {intersection(alzheimers, cancer)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
