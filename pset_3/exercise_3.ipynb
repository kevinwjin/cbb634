{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Two-dimensional gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D gradient descent (adapted from Slides 8)\n",
    "def one_dim_gd(f, delta, learning_rate, guess, iterations):\n",
    "    fprime = lambda x: (f(x + delta) - f(x)) / delta\n",
    "    for _ in range(iterations):\n",
    "        print(guess)\n",
    "        guess = guess - learning_rate * fprime(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "-0.20027000431855413\n",
      "1.4130210876611642\n",
      "1.2434441591772032\n",
      "1.1255487044080574\n",
      "1.0578788670328958\n",
      "1.024779984532536\n",
      "1.0101887274510606\n",
      "1.004097799830344\n",
      "1.001617715333463\n"
     ]
    }
   ],
   "source": [
    "# Test 1-D gradient descent\n",
    "g = lambda x: (x ** 4 / 4 - 2 * x ** 3 / 3 - x ** 2 / 2 + 2 * x + 2) # A function g(x)\n",
    "f = lambda x: g(x - 2) # A composite function f(x)\n",
    "delta = 1e-4 # Difference quotient (h)\n",
    "learning_rate = 0.1 # Learning rate (gamma)\n",
    "guess = 7 # Initial guess\n",
    "iterations = 10 # Number of iterations\n",
    "\n",
    "one_dim_gd(f, delta, learning_rate, guess, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-D gradient descent (modified for black-box optimization)\n",
    "import requests\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "url = \"http://ramcdougal.com/cgi-bin/error_function.py\"\n",
    "\n",
    "def two_dim_gd(delta, learning_rate, guess_a, guess_b, iterations):\n",
    "    for _ in range(iterations):\n",
    "        # Retrieve intial error for current guess_a and guess_b\n",
    "        error = float(requests.get(url, params = {'a': guess_a, 'b': guess_b}, headers = {\"User-Agent\": user_agent}).text)\n",
    "\n",
    "        # Estimate the gradients using finite differences method\n",
    "        error_a_delta = float(requests.get(url, params = {'a': guess_a + delta, 'b': guess_b}, headers = {\"User-Agent\": user_agent}).text)\n",
    "        error_b_delta = float(requests.get(url, params = {'a': guess_a, 'b': guess_b + delta}, headers = {\"User-Agent\": user_agent}).text)\n",
    "\n",
    "        grad_a = (error_a_delta - error) / delta\n",
    "        grad_b = (error_b_delta - error) / delta\n",
    "\n",
    "        # Update guess_a and guess_b\n",
    "        guess_a = guess_a - learning_rate * grad_a\n",
    "        guess_b = guess_b - learning_rate * grad_b\n",
    "\n",
    "        # Monitor convergence\n",
    "        print(f\"Iteration {_ + 1}: Error = {error}\")\n",
    "\n",
    "    # Return final optimized values    \n",
    "    return guess_a, guess_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** The function we are interested in, the error function, is a black-box. As such, we cannot take its derivative and cannot directly calculate the gradient. We circumvent this by approximating the gradient using the method of finite differences, in which we choose a small value (known as $h$ or delta) to move the function by. We subtract the difference between the new function position and the original function position and divide the difference by $h$. This gives us an estimate of the derivative that improves as $h$ gets smaller (the estimate will approach the actual derivative as $h$ tends to zero). To extend this to two dimensions, we perform this operation once for each parameter. The stopping criterion was determined by visual confirmation of convergence of the function position, which in this case is the error. After each iteration of gradient descent, I print out the new error and determine manually when convergence is reached. Numerical choices made include careful selection of $h$, the learning rate, and the number of iterations. $h$ should be as small as possible to achieve a close approximation of the gradient while also being large enough to be computationally efficient. The same careful balance applies to the learning rate, as a smaller learning rate will be inefficient and require more iterations, while a larger learning rate might overshoot the minimum entirely or cause the gradient descent algorithm to oscillate around the minimum. As for the number of iterations, it should be sufficiently large enough to capture convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Error = 1.216377\n",
      "Iteration 2: Error = 1.1744797602\n",
      "Iteration 3: Error = 1.14766583105\n",
      "Iteration 4: Error = 1.13050515985\n",
      "Iteration 5: Error = 1.11952252504\n",
      "Iteration 6: Error = 1.11249379433\n",
      "Iteration 7: Error = 1.10799553138\n",
      "Iteration 8: Error = 1.10511674237\n",
      "Iteration 9: Error = 1.10327439756\n",
      "Iteration 10: Error = 1.10209536131\n",
      "Iteration 11: Error = 1.10134082865\n",
      "Iteration 12: Error = 1.10085796859\n",
      "Iteration 13: Error = 1.10054897118\n",
      "Iteration 14: Error = 1.10035123887\n",
      "Iteration 15: Error = 1.10022471098\n",
      "Iteration 16: Error = 1.10014374993\n",
      "Iteration 17: Error = 1.1000919481\n",
      "Iteration 18: Error = 1.10005880577\n",
      "Iteration 19: Error = 1.10003760326\n",
      "Iteration 20: Error = 1.10002404048\n",
      "Iteration 21: Error = 1.10001536578\n",
      "Iteration 22: Error = 1.10000981837\n",
      "Iteration 23: Error = 1.10000627153\n",
      "Iteration 24: Error = 1.10000400434\n",
      "Iteration 25: Error = 1.1000025556\n",
      "Iteration 26: Error = 1.10000163019\n",
      "Iteration 27: Error = 1.10000103937\n",
      "Iteration 28: Error = 1.1000006624\n",
      "Iteration 29: Error = 1.10000042205\n",
      "Iteration 30: Error = 1.10000026897\n",
      "Iteration 31: Error = 1.10000017159\n",
      "Iteration 32: Error = 1.10000010973\n",
      "Iteration 33: Error = 1.10000007052\n",
      "Iteration 34: Error = 1.10000004573\n",
      "Iteration 35: Error = 1.1000000301\n",
      "Iteration 36: Error = 1.10000002029\n",
      "Iteration 37: Error = 1.10000001417\n",
      "Iteration 38: Error = 1.10000001038\n",
      "Iteration 39: Error = 1.10000000804\n",
      "Iteration 40: Error = 1.10000000663\n",
      "Iteration 41: Error = 1.10000000579\n",
      "Iteration 42: Error = 1.10000000531\n",
      "Iteration 43: Error = 1.10000000503\n",
      "Iteration 44: Error = 1.10000000489\n",
      "Iteration 45: Error = 1.10000000483\n",
      "Iteration 46: Error = 1.10000000481\n",
      "Iteration 47: Error = 1.10000000481\n",
      "Iteration 48: Error = 1.10000000483\n",
      "Iteration 49: Error = 1.10000000484\n",
      "Iteration 50: Error = 1.10000000487\n",
      "Optimized a: 0.21595406000068884, b: 0.6889473100002981\n"
     ]
    }
   ],
   "source": [
    "# Minimize the black-box error function\n",
    "delta = 1e-4 # Difference quotient (h)\n",
    "learning_rate = 0.1 # Learning rate (gamma)\n",
    "guess_a, guess_b = 0.5, 0.5 # Initial guesses\n",
    "iterations = 50 # Number of iterations\n",
    "\n",
    "a, b = two_dim_gd(delta, learning_rate, guess_a, guess_b, iterations)\n",
    "print(f\"Optimized a: {a}, b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
